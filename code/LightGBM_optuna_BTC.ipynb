{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"/data/ephemeral/home/BTC/data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:02<00:00, 38.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Explanatory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함\n",
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\": \"_type\",\n",
    "    \"hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close\": \"close\",\n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "    \"hourly_network-data_difficulty_difficulty\": \"difficulty\",\n",
    "    \"hourly_network-data_supply_supply_total\": \"supply_total\",\n",
    "    \"hourly_network-data_utxo-count_utxo_count\": \"utxo_count\"\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous 열을 따로 할당해둠\n",
    "conti_cols: List[str] = [\n",
    "    \"close\",\n",
    "    \"open_interest\",\n",
    "    \"difficulty\",\n",
    "    \"supply_total\",\n",
    "    \"utxo_count\"\n",
    "]\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")\n",
    "\n",
    "# concat 하여 df 에 할당\n",
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _type에 따라 train, test 분리\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "### LightGBM + GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "\n",
    "X_train = train_df.drop([\"ID\", \"target\", \"close\"], axis=1)\n",
    "y_train = train_df[\"close\"]\n",
    "X_test = test_df.drop([\"ID\", \"target\", \"close\"], axis=1)\n",
    "y_test = test_df[\"close\"]\n",
    "target = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_to_class(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"close 변수를 target값으로 변환하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): 변환을 원하는 close 변수\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: 변환된 target 값\n",
    "    \"\"\"\n",
    "    close = pd.DataFrame()\n",
    "    close['close'] = series\n",
    "    close['close_lag1'] = close['close'].shift(1)\n",
    "    close['close_lag1_percent'] = (close['close'] - close['close_lag1']) / close['close_lag1']\n",
    "    close['class'] = close['close']\n",
    "    for i in range(close.shape[0]):\n",
    "        if close.loc[i, 'close_lag1_percent'] < -0.005:\n",
    "            close.loc[i, 'class'] = 0\n",
    "        elif close.loc[i, 'close_lag1_percent'] < 0:\n",
    "            close.loc[i, 'class'] = 1\n",
    "        elif close.loc[i, 'close_lag1_percent'] < 0.005:\n",
    "            close.loc[i, 'class'] = 2\n",
    "        else:\n",
    "            close.loc[i, 'class'] = 3\n",
    "            \n",
    "    return close['class'].shift(-1).fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def evaluate(valid_target, y_valid, y_pred, metric):\n",
    "    if metric == 'accuracy':\n",
    "        classes_pred = close_to_class(y_pred)\n",
    "        return accuracy_score(valid_target, classes_pred)\n",
    "    if metric == 'mae':\n",
    "        return mean_absolute_error(y_valid, y_pred)\n",
    "    if metric == \"mse\":\n",
    "        return mean_squared_error(y_valid, y_pred)\n",
    "    if metric == \"mape\":\n",
    "        mae = mean_absolute_percentage_error(y_valid, y_pred)\n",
    "        return mae / np.mean(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, cv, metric):\n",
    "    #kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    kfold = KFold(n_splits=cv)\n",
    "    score_list = []\n",
    "    for train_index, valid_index in kfold.split(X_train):\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_valid, y_valid = X_train.iloc[valid_index], y_train.iloc[valid_index]\n",
    "        \n",
    "        vaild_target = target.iloc[valid_index]\n",
    "\n",
    "        # preprocessing\n",
    "        X_train_fold.fillna(X_train_fold.mean(), inplace=True)\n",
    "        y_train_fold.fillna(y_train_fold.mean(), inplace=True) \n",
    "        X_valid.fillna(X_valid.mean(), inplace=True)\n",
    "        y_valid.fillna(y_valid.mean(), inplace=True)  # 이 부분을 mice와 같은 방법으로 조정할 예정. feature selection 등도 여기에서.\n",
    "\n",
    "        #model.fit(X_train_fold.drop(\"target\", axis=1), y_train_fold)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        #y_pred = model.predict(X_valid.drop(\"target\", axis=1))\n",
    "        y_pred = model.predict(X_valid)\n",
    "        score = evaluate(vaild_target, y_valid, y_pred, metric=metric)\n",
    "        score_list.append(score)\n",
    "\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 하이퍼파라미터 설정\n",
    "    params = {\n",
    "        \"boosting_type\" : \"gbdt\",\n",
    "        \"metric\" : \"mean_squared_error\",\n",
    "        \"verbose\" : -1,\n",
    "        \"num_leaves\" : trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "        \"learning_rate\" : trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 100),\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", 3, 10), # 과적합 방지\n",
    "        \"min_child_weight\" : trial.suggest_int(\"min_child_weight\", 1, 10), # 과소적합 방지\n",
    "        \"subsample\" : trial.suggest_uniform(\"subsample\", 0.5, 1.0), # 데이터 샘플링 비율, 과적합 방지\n",
    "        \"colsample_bytree\" : trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        # \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-4, 10.0),\n",
    "        # \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-4, 10.0),\n",
    "        \"random_state\" : 42,\n",
    "        \"force_col_wise\": True,\n",
    "        #\"device\" : \"gpu\", \n",
    "    }\n",
    "\n",
    "    lgb_model = lgb.LGBMRegressor(**params)\n",
    "    acc = model_train(lgb_model, X_train, y_train, cv=5, metric=\"accuracy\")\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 18:26:14,476] A new study created in memory with name: no-name-afb137f1-01fd-4a0f-80e8-850180f20516\n",
      "[I 2024-09-24 18:26:19,228] Trial 0 finished with value: 0.38789954337899546 and parameters: {'num_leaves': 73, 'learning_rate': 0.01115435422614097, 'n_estimators': 73, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.9052745886939833, 'colsample_bytree': 0.8747326112554548}. Best is trial 0 with value: 0.38789954337899546.\n",
      "[I 2024-09-24 18:26:22,003] Trial 1 finished with value: 0.4094748858447489 and parameters: {'num_leaves': 30, 'learning_rate': 0.0031428113742668355, 'n_estimators': 43, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7395567432021037, 'colsample_bytree': 0.5334275566344353}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:25,055] Trial 2 finished with value: 0.38515981735159815 and parameters: {'num_leaves': 42, 'learning_rate': 0.0584399351689488, 'n_estimators': 27, 'max_depth': 9, 'min_child_weight': 4, 'subsample': 0.6594211945961406, 'colsample_bytree': 0.8548298008153152}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:29,807] Trial 3 finished with value: 0.386986301369863 and parameters: {'num_leaves': 64, 'learning_rate': 0.014630602311508074, 'n_estimators': 99, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.5572282927825023, 'colsample_bytree': 0.8245043155793158}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:34,405] Trial 4 finished with value: 0.395662100456621 and parameters: {'num_leaves': 89, 'learning_rate': 0.052722445609470585, 'n_estimators': 89, 'max_depth': 8, 'min_child_weight': 2, 'subsample': 0.7706508842223809, 'colsample_bytree': 0.5487142975492014}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:38,182] Trial 5 finished with value: 0.3835616438356165 and parameters: {'num_leaves': 23, 'learning_rate': 0.01900136188292875, 'n_estimators': 87, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.8264072849168478, 'colsample_bytree': 0.8842394089264313}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:41,771] Trial 6 finished with value: 0.39303652968036523 and parameters: {'num_leaves': 37, 'learning_rate': 0.011785755352619517, 'n_estimators': 58, 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.7143459941309278, 'colsample_bytree': 0.7131901799340694}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:45,187] Trial 7 finished with value: 0.4026255707762557 and parameters: {'num_leaves': 26, 'learning_rate': 0.002504354165424602, 'n_estimators': 88, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8512461943834193, 'colsample_bytree': 0.626086699722809}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:48,295] Trial 8 finished with value: 0.4020547945205479 and parameters: {'num_leaves': 63, 'learning_rate': 0.005604048822010286, 'n_estimators': 73, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.832376902767268, 'colsample_bytree': 0.9352978846449664}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:50,987] Trial 9 finished with value: 0.39783105022831056 and parameters: {'num_leaves': 52, 'learning_rate': 0.020852057300456528, 'n_estimators': 17, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.9090322972499008, 'colsample_bytree': 0.8385290884175804}. Best is trial 1 with value: 0.4094748858447489.\n",
      "[I 2024-09-24 18:26:53,686] Trial 10 finished with value: 0.41700913242009124 and parameters: {'num_leaves': 96, 'learning_rate': 0.001357292476471489, 'n_estimators': 38, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.9937260372217247, 'colsample_bytree': 0.5315747336736791}. Best is trial 10 with value: 0.41700913242009124.\n",
      "[I 2024-09-24 18:26:56,362] Trial 11 finished with value: 0.4167808219178083 and parameters: {'num_leaves': 96, 'learning_rate': 0.001136337074208281, 'n_estimators': 31, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.6239128426125855, 'colsample_bytree': 0.5042206739103756}. Best is trial 10 with value: 0.41700913242009124.\n",
      "[I 2024-09-24 18:26:59,108] Trial 12 finished with value: 0.4171232876712329 and parameters: {'num_leaves': 98, 'learning_rate': 0.0010222249818206935, 'n_estimators': 44, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.500124818608134, 'colsample_bytree': 0.6250279686741511}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:01,853] Trial 13 finished with value: 0.4166666666666667 and parameters: {'num_leaves': 85, 'learning_rate': 0.0011026984847620574, 'n_estimators': 42, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.9750022182337255, 'colsample_bytree': 0.6439239441077116}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:05,263] Trial 14 finished with value: 0.4019406392694064 and parameters: {'num_leaves': 100, 'learning_rate': 0.0020079944807510053, 'n_estimators': 53, 'max_depth': 6, 'min_child_weight': 8, 'subsample': 0.5353341095718052, 'colsample_bytree': 0.6170027423929115}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:07,972] Trial 15 finished with value: 0.41141552511415524 and parameters: {'num_leaves': 79, 'learning_rate': 0.00378321584575402, 'n_estimators': 36, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.9982472581368451, 'colsample_bytree': 0.7175029441546208}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:10,617] Trial 16 finished with value: 0.4101598173515981 and parameters: {'num_leaves': 92, 'learning_rate': 0.0015959708496252716, 'n_estimators': 11, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.5018099985180403, 'colsample_bytree': 0.5939291825547977}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:13,527] Trial 17 finished with value: 0.40719178082191776 and parameters: {'num_leaves': 74, 'learning_rate': 0.005567312801193783, 'n_estimators': 55, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.6084816492076486, 'colsample_bytree': 0.6910216093969102}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:16,563] Trial 18 finished with value: 0.386986301369863 and parameters: {'num_leaves': 84, 'learning_rate': 0.09966027779974691, 'n_estimators': 23, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.6877522074378627, 'colsample_bytree': 0.7760081594046272}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:19,352] Trial 19 finished with value: 0.41301369863013704 and parameters: {'num_leaves': 71, 'learning_rate': 0.005789921480016347, 'n_estimators': 63, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.7759234775544642, 'colsample_bytree': 0.5698948707190452}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:22,980] Trial 20 finished with value: 0.40616438356164386 and parameters: {'num_leaves': 56, 'learning_rate': 0.0010158081328889058, 'n_estimators': 45, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.9390125592898216, 'colsample_bytree': 0.9973312366173952}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:25,679] Trial 21 finished with value: 0.4166666666666667 and parameters: {'num_leaves': 99, 'learning_rate': 0.0014691631632676367, 'n_estimators': 36, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.619331636174965, 'colsample_bytree': 0.5030118108084538}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:28,406] Trial 22 finished with value: 0.4147260273972603 and parameters: {'num_leaves': 94, 'learning_rate': 0.0015141236020316297, 'n_estimators': 27, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.5910797635475498, 'colsample_bytree': 0.5013897561215068}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:31,203] Trial 23 finished with value: 0.41609589041095896 and parameters: {'num_leaves': 100, 'learning_rate': 0.0010179302502675638, 'n_estimators': 33, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.6532957939827206, 'colsample_bytree': 0.6618146579417739}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:34,060] Trial 24 finished with value: 0.4107305936073059 and parameters: {'num_leaves': 83, 'learning_rate': 0.0022796712354826816, 'n_estimators': 49, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.523715652243306, 'colsample_bytree': 0.5722629482025663}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:36,769] Trial 25 finished with value: 0.4039954337899543 and parameters: {'num_leaves': 91, 'learning_rate': 0.0040812798631096135, 'n_estimators': 19, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.5510068382167709, 'colsample_bytree': 0.5366904715401872}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:39,454] Trial 26 finished with value: 0.41689497716894974 and parameters: {'num_leaves': 94, 'learning_rate': 0.0016926453766333345, 'n_estimators': 31, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.5842621243374067, 'colsample_bytree': 0.5953539373511254}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:42,250] Trial 27 finished with value: 0.4122146118721461 and parameters: {'num_leaves': 80, 'learning_rate': 0.0017865479890726552, 'n_estimators': 39, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.5711233309268507, 'colsample_bytree': 0.672910312958179}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:45,198] Trial 28 finished with value: 0.4127853881278539 and parameters: {'num_leaves': 88, 'learning_rate': 0.0028755454465322658, 'n_estimators': 64, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.6558520294458857, 'colsample_bytree': 0.6067458646847733}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:48,595] Trial 29 finished with value: 0.40011415525114147 and parameters: {'num_leaves': 70, 'learning_rate': 0.007434294192577731, 'n_estimators': 49, 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.8739910226660536, 'colsample_bytree': 0.7391048248078751}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:51,610] Trial 30 finished with value: 0.4069634703196347 and parameters: {'num_leaves': 75, 'learning_rate': 0.0014102459562864932, 'n_estimators': 29, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.5039910724511409, 'colsample_bytree': 0.7906545369056136}. Best is trial 12 with value: 0.4171232876712329.\n",
      "[I 2024-09-24 18:27:54,377] Trial 31 finished with value: 0.4172374429223744 and parameters: {'num_leaves': 92, 'learning_rate': 0.001337931131470499, 'n_estimators': 34, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.5899659159647938, 'colsample_bytree': 0.5793507918196845}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:27:57,220] Trial 32 finished with value: 0.41187214611872147 and parameters: {'num_leaves': 95, 'learning_rate': 0.0019524875387555314, 'n_estimators': 45, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.5949991933205555, 'colsample_bytree': 0.5834549574046262}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:27:59,877] Trial 33 finished with value: 0.4166666666666667 and parameters: {'num_leaves': 88, 'learning_rate': 0.0013433438942713888, 'n_estimators': 22, 'max_depth': 3, 'min_child_weight': 9, 'subsample': 0.5736007245994916, 'colsample_bytree': 0.6500462141217473}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:02,684] Trial 34 finished with value: 0.4109589041095891 and parameters: {'num_leaves': 95, 'learning_rate': 0.0029852593346036036, 'n_estimators': 39, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7310568378130258, 'colsample_bytree': 0.5417300261565833}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:05,327] Trial 35 finished with value: 0.41700913242009136 and parameters: {'num_leaves': 45, 'learning_rate': 0.002148268859850206, 'n_estimators': 12, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.6929040451520437, 'colsample_bytree': 0.5655711869741616}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:07,968] Trial 36 finished with value: 0.4134703196347032 and parameters: {'num_leaves': 32, 'learning_rate': 0.0038672496634715847, 'n_estimators': 15, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7985842213469326, 'colsample_bytree': 0.555190548716364}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:11,629] Trial 37 finished with value: 0.39600456621004565 and parameters: {'num_leaves': 51, 'learning_rate': 0.002257525977456133, 'n_estimators': 62, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.6999323503820011, 'colsample_bytree': 0.528426122163903}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:14,219] Trial 38 finished with value: 0.4172374429223744 and parameters: {'num_leaves': 43, 'learning_rate': 0.0012292018918950223, 'n_estimators': 10, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6745708418190773, 'colsample_bytree': 0.6279455269031692}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:16,937] Trial 39 finished with value: 0.40422374429223745 and parameters: {'num_leaves': 44, 'learning_rate': 0.02672746421671316, 'n_estimators': 10, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.6836524128870475, 'colsample_bytree': 0.6382655014290121}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:19,566] Trial 40 finished with value: 0.41255707762557076 and parameters: {'num_leaves': 45, 'learning_rate': 0.00829552809279975, 'n_estimators': 14, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.6459190289313732, 'colsample_bytree': 0.6939228282407944}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:22,306] Trial 41 finished with value: 0.41700913242009136 and parameters: {'num_leaves': 39, 'learning_rate': 0.0013049080506381944, 'n_estimators': 23, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.7665873697316826, 'colsample_bytree': 0.5648079065941675}. Best is trial 31 with value: 0.4172374429223744.\n",
      "[I 2024-09-24 18:28:24,974] Trial 42 finished with value: 0.41735159817351597 and parameters: {'num_leaves': 40, 'learning_rate': 0.001241478711853387, 'n_estimators': 23, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7555628202792165, 'colsample_bytree': 0.6112268450129043}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:27,591] Trial 43 finished with value: 0.41712328767123286 and parameters: {'num_leaves': 33, 'learning_rate': 0.002548857148529046, 'n_estimators': 19, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7380167978200795, 'colsample_bytree': 0.6223770487902476}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:31,600] Trial 44 finished with value: 0.41689497716894974 and parameters: {'num_leaves': 32, 'learning_rate': 0.0010032992087092032, 'n_estimators': 20, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7247707812735442, 'colsample_bytree': 0.6296522123487007}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:34,308] Trial 45 finished with value: 0.4133561643835616 and parameters: {'num_leaves': 20, 'learning_rate': 0.0027307755907976165, 'n_estimators': 25, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.7464859119241203, 'colsample_bytree': 0.6197408196925952}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:37,640] Trial 46 finished with value: 0.4044520547945206 and parameters: {'num_leaves': 37, 'learning_rate': 0.0012752089987064861, 'n_estimators': 17, 'max_depth': 9, 'min_child_weight': 1, 'subsample': 0.7944566311797641, 'colsample_bytree': 0.6706896909891221}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:40,449] Trial 47 finished with value: 0.40605022831050225 and parameters: {'num_leaves': 27, 'learning_rate': 0.001706359763505826, 'n_estimators': 27, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.6692982620573767, 'colsample_bytree': 0.6922615978788836}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:43,330] Trial 48 finished with value: 0.41324200913242015 and parameters: {'num_leaves': 50, 'learning_rate': 0.0012212005489365962, 'n_estimators': 75, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.7096353453681785, 'colsample_bytree': 0.6069117462350972}. Best is trial 42 with value: 0.41735159817351597.\n",
      "[I 2024-09-24 18:28:46,127] Trial 49 finished with value: 0.40273972602739716 and parameters: {'num_leaves': 40, 'learning_rate': 0.03395477421308807, 'n_estimators': 34, 'max_depth': 4, 'min_child_weight': 6, 'subsample': 0.6371748366315213, 'colsample_bytree': 0.7290986252565117}. Best is trial 42 with value: 0.41735159817351597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'num_leaves': 40, 'learning_rate': 0.001241478711853387, 'n_estimators': 23, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7555628202792165, 'colsample_bytree': 0.6112268450129043}\n"
     ]
    }
   ],
   "source": [
    "# optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# best params\n",
    "print(\"Best Hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 모델\n",
    "best_params = study.best_params\n",
    "best_lgb_model = lgb.LGBMRegressor(**best_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24801\n",
      "[LightGBM] [Info] Number of data points in the train set: 7008, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 30562.564983\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24849\n",
      "[LightGBM] [Info] Number of data points in the train set: 7008, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 29000.900340\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24849\n",
      "[LightGBM] [Info] Number of data points in the train set: 7008, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 28793.712531\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24825\n",
      "[LightGBM] [Info] Number of data points in the train set: 7008, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 29236.290875\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24825\n",
      "[LightGBM] [Info] Number of data points in the train set: 7008, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score 26393.087751\n",
      "LGB model accuracy: 0.41735159817351597\n"
     ]
    }
   ],
   "source": [
    "avg = model_train(best_lgb_model, X_train, y_train, cv=5, metric=\"accuracy\")\n",
    "\n",
    "print(\"LGB model accuracy:\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop([\"ID\", \"target\", \"close\"], axis=1)\n",
    "y_test = test_df[\"close\"]\n",
    "\n",
    "# 결측치 처리\n",
    "X_test.fillna(X_test.mean(), inplace=True)\n",
    "y_test.fillna(y_test.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb predict\n",
    "y_test_pred = best_lgb_model.predict(X_test)\n",
    "y_test_pred_class = close_to_class(y_test_pred) # 예측 결과를 범주형 변수로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file 할당후 save \n",
    "submission_df = submission_df.assign(target = y_test_pred_class)\n",
    "submission_df[\"target\"] = submission_df[\"target\"].astype(np.int8)\n",
    "submission_df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
