{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "import optuna\n",
    "from data_preprocessing import *\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"/data/ephemeral/home/BTC/data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Explanatory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함\n",
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\" : \"_type\",\n",
    "\n",
    "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
    "\n",
    "    # \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
    "    # \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
    "    # \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
    "    # \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
    "    \n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "\n",
    "    \"hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close\": \"close\",\n",
    "    \n",
    "    # \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"taker_buy_volume\",\n",
    "    # \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"taker_sell_volume\",\n",
    "    # \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"taker_buy_ratio\",\n",
    "    # \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"taker_sell_ratio\",\n",
    "    # \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"taker_buy_sell_ratio\",\n",
    "\n",
    "    # \"hourly_network-data_addresses-count_addresses_count_active\": \"addresses_count_active\",\n",
    "    # \"hourly_network-data_addresses-count_addresses_count_sender\": \"addresses_count_sender\",\n",
    "    # \"hourly_network-data_addresses-count_addresses_count_receiver\": \"addresses_count_receiver\",\n",
    "\n",
    "    # \"hourly_network-data_block-bytes_block_bytes\": \"block_bytes\",\n",
    "\n",
    "    # \"hourly_network-data_block-count_block_count\": \"block_count\",\n",
    "    \n",
    "    # \"hourly_network-data_block-interval_block_interval\": \"block_interval\",\n",
    "\n",
    "    # \"hourly_network-data_blockreward_blockreward\": \"blockreward\",\n",
    "    # \"hourly_network-data_blockreward_blockreward_usd\": \"blockreward_usd\",\n",
    "    \n",
    "    \"hourly_network-data_difficulty_difficulty\": \"difficulty\",\n",
    "\n",
    "    # \"hourly_network-data_fees_fees_block_mean\": \"fees_block_mean\",\n",
    "    \"hourly_network-data_fees_fees_block_mean_usd\": \"fees_block_mean_usd\",\n",
    "    # \"hourly_network-data_fees_fees_total\": \"fees_total\",\n",
    "    # \"hourly_network-data_fees_fees_total_usd\": \"fees_total_usd\",\n",
    "    # \"hourly_network-data_fees_fees_reward_percent\": \"fees_reward_percent\",\n",
    "\n",
    "    # \"hourly_network-data_fees-transaction_fees_transaction_mean\": \"fees_transaction_mean\",\n",
    "    \"hourly_network-data_fees-transaction_fees_transaction_mean_usd\": \"fees_transaction_mean_usd\",\n",
    "    # \"hourly_network-data_fees-transaction_fees_transaction_median\": \"fees_transaction_median\",\n",
    "    # \"hourly_network-data_fees-transaction_fees_transaction_median_usd\": \"fees_transaction_median_usd\",\n",
    "\n",
    "    # \"hourly_network-data_hashrate_hashrate\": \"hashrate\",\n",
    "\n",
    "    \"hourly_network-data_supply_supply_total\": \"supply_total\",\n",
    "    # \"hourly_network-data_supply_supply_new\": \"supply_new\",\n",
    "\n",
    "    # \"hourly_network-data_tokens-transferred_tokens_transferred_total\": \"tokens_transferred_total\",\n",
    "    # \"hourly_network-data_tokens-transferred_tokens_transferred_mean\": \"tokens_transferred_mean\",\n",
    "    # \"hourly_network-data_tokens-transferred_tokens_transferred_median\": \"tokens_transferred_median\",\n",
    "\n",
    "    # \"hourly_network-data_transactions-count_transactions_count_total\": \"transactions_count_total\",\n",
    "    \"hourly_network-data_transactions-count_transactions_count_mean\": \"transactions_count_mean\",\n",
    "\n",
    "    \"hourly_network-data_utxo-count_utxo_count\": \"utxo_count\",\n",
    "    \n",
    "    \"hourly_network-data_velocity_velocity_supply_total\": \"velocity_supply_total\"\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous 열을 따로 할당해둠\n",
    "conti_cols: List[str] = [\n",
    "    \"close\",\n",
    "    \"open_interest\",\n",
    "    \"difficulty\",\n",
    "    \"supply_total\",\n",
    "    \"utxo_count\"\n",
    "]\n",
    "\n",
    "# # 최대 24시간의 shift 피쳐를 계산\n",
    "# shift_list = shift_feature(\n",
    "#     df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    "# )\n",
    "\n",
    "# # concat 하여 df 에 할당\n",
    "# df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _type에 따라 train, test 분리\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost 라이브러리에 구현되어 있는 XGBRegressor 모델을 사용하여 학습 및 평가를 진행합니다. xgboost의 래퍼 클래스(wrapper class) 중 **사이킷런 래퍼**를 사용할 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model parameter (XGBRegressor)\n",
    "* n_estimator: 트리의 개수 (디폴트 = 100)  \n",
    "\n",
    "* learning_rate: 학습 단계별 가중치를 얼마나 사용할지(이전 결과를 얼마나 반영할 것인지) 결정. 일반적으로 0.01 ~ 0.2\n",
    "\n",
    "* max_depth: 트리의 최대 깊이. (디폴트 = 6) 일반적으로 3 ~ 10  \n",
    "\n",
    "* min_child_weight: child에서 필요한 모든 관측치에 대한 가중치의 최소 합. 이 값보다 샘플 수가 작으면 leaf node가 된다. 너무 큰 값을 적용하면 과소적합이 될 수 있다.  \n",
    "\n",
    "* early stopping_rounds: 최대한 몇 개의 트리를 완성해볼 것인지 결정. valid loss에 더 이상 진전이 없으면 멈춘다. n_estimator가 높을 때 주로 사용  \n",
    "\n",
    "* gamma: 트리에서 추가적으로 가지를 나눌지를 결정할 최소 손실 감소값. 값이 클수록 과적합 감소 효과  \n",
    "\n",
    "* subsample: 각 트리마다 데이터 샘플링 비율. (디폴트 = 1) 일반적으로 0.5 ~ 1  \n",
    "\n",
    "* colsample_bytree: 각 트리마다 feature 샘플링 비율. (디폴트 = 1) 일반적으로 0.5 ~ 1  \n",
    "\n",
    "* reg_lambda: L2 regularization 가중치 (디폴트 = 1)  \n",
    "\n",
    "* reg_alpha: L1 regularization 가중치 (디폴트 = 1)  \n",
    "\n",
    "* scale_pos_weight: 데이터가 불균형할때 사용, 0보다 큰 값. (디폴트 = 1) 보통 값을 (음성 데이터 수)/(양성 데이터 수) 값으로 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit 파라미터\n",
    "\n",
    "* early_stopping_rounds:\n",
    "* eval_metric: \n",
    "* eval_set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop([\"ID\", \"target\", \"close\"], axis=1)\n",
    "y_train = train_df[\"close\"]\n",
    "target = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_to_class(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"close 변수를 target값으로 변환하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): 변환을 원하는 close 변수\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: 변환된 target 값\n",
    "    \"\"\"\n",
    "    close = pd.DataFrame()\n",
    "    close['close'] = series\n",
    "    close['close_lag1'] = close['close'].shift(1)\n",
    "    close['close_lag1_percent'] = (close['close'] - close['close_lag1']) / close['close_lag1']\n",
    "    close['class'] = close['close']\n",
    "    for i in range(close.shape[0]):\n",
    "        if close.loc[i, 'close_lag1_percent'] < -0.005:\n",
    "            close.loc[i, 'class'] = 0\n",
    "        elif close.loc[i, 'close_lag1_percent'] < 0:\n",
    "            close.loc[i, 'class'] = 1\n",
    "        elif close.loc[i, 'close_lag1_percent'] < 0.005:\n",
    "            close.loc[i, 'class'] = 2\n",
    "        else:\n",
    "            close.loc[i, 'class'] = 3\n",
    "            \n",
    "    return close[\"class\"].shift(-1).fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "def evaluate(valid_target: pd.Series, \n",
    "             y_valid: pd.Series, \n",
    "             y_pred: np.ndarray, \n",
    "             metric: str\n",
    ") -> float:\n",
    "    \"\"\"평가지표 metric을 반환하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        valid_target: (pd.Series): k-fold로 분할한 target의 검증 데이터\n",
    "        y_valid (pd.Series): k-fold로 분할한 close의 검증 데이터\n",
    "        y_pred (np.ndarray): 모델을 사용하여 예측한 변수\n",
    "        metric (str): 사용할 평가지표 metric 이름\n",
    "\n",
    "    Returns:\n",
    "        float: 사용할 평가지표 metric 값\n",
    "    \"\"\"\n",
    "    if metric == \"accuracy\":\n",
    "        y_pred_class = close_to_class(y_pred)\n",
    "        return accuracy_score(valid_target, y_pred_class)\n",
    "    elif metric == \"mae\":\n",
    "        return mean_absolute_error(y_valid, y_pred)\n",
    "    elif metric == \"mse\":\n",
    "        return mean_squared_error(y_valid, y_pred)\n",
    "    elif metric == \"mape\":\n",
    "        return mean_absolute_percentage_error(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model: Any, \n",
    "                X_train: pd.DataFrame, \n",
    "                y_train: pd.Series, \n",
    "                cv: int, \n",
    "                metric: str, \n",
    ") -> Tuple[Any, float]:\n",
    "    \"\"\"K-Fold로 데이터를 분할한 후 전처리를 거쳐 주어진 모델로 데이터를 학습 및 평가를 진행합니다.\n",
    "\n",
    "    Args:\n",
    "        model (Any): 사용하는 모델 객체\n",
    "        X_train (pd.DataFrame): 설명변수로 이루어진 학습 데이터프레임\n",
    "        y_train (pd.Seris): 예측변수로 이루어진 학습 시리즈\n",
    "        cv (int): 교차검증시 분할할 폴드의 수\n",
    "        metric (str): 사용할 평가지표 metric 이름\n",
    "\n",
    "    Returns:\n",
    "        Any, float: 폴드 내에서 가장 평가지표 값이 높은 모델 객체, 평가지표 metric 값\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=cv)\n",
    "    score_list = []\n",
    "    fold_model = []\n",
    "    \n",
    "    # warm_start는 모델의 속성으로, 같은 모델을 반복 학습할 때 이전 학습에서 학습된 파라미터를 초기화하지 않고 이어서 학습을 진행하는 옵션\n",
    "    if hasattr(model, \"warm_start\"):\n",
    "        model.warm_start = True\n",
    "\n",
    "    # K-Fold 교차 검증\n",
    "    for train_index, valid_index in kfold.split(X_train):\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "        X_valid, y_valid = X_train.iloc[valid_index], y_train.iloc[valid_index]\n",
    "\n",
    "        valid_target = target[valid_index]\n",
    "        \n",
    "        # 전처리\n",
    "        fill_feature(X_train_fold, method=\"mean\")\n",
    "        fill_feature(X_valid, method=\"mean\")        \n",
    "        \n",
    "        # 모델 학습\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        fold_model.append(model)\n",
    "\n",
    "        y_pred = model.predict(X_valid)\n",
    "        score = evaluate(valid_target, y_valid, y_pred, metric=metric)  # 평가지표 metric 반환\n",
    "        score_list.append(score)\n",
    "    \n",
    "    return fold_model[np.argmax(score_list)], np.max(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: int) -> float:\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 5),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"device\": \"gpu\",\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    xgb_model = XGBRegressor(**params)\n",
    "    _, acc = model_train(xgb_model, X_train, y_train, cv=5, metric=\"accuracy\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna study 생성 및 최적화 실행\n",
    "# random_state를 설정한 Sampler 사용\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters: \", study.best_params)\n",
    "\n",
    "# 최적의 하이퍼파라미터를 사용하여 최종 모델 생성\n",
    "best_params = study.best_params\n",
    "best_params[\"device\"] = \"gpu\"\n",
    "best_params[\"random_state\"] = 42\n",
    "best_xgb_model = XGBRegressor(**best_params)\n",
    "\n",
    "# 최종 모델 평가\n",
    "fold_best_xgb_model, acc = model_train(best_xgb_model, X_train, y_train, cv=5, metric=\"accuracy\")\n",
    "print(f\"XGBoost model accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_best_xgb_model이 학습 데이터 전체를 학습할 수 있도록 결측치 처리\n",
    "X_train = fill_feature(X_train, method=\"mean\")\n",
    "X_test = test_df.drop([\"ID\", \"target\", \"close\"], axis=1)\n",
    "X_test = fill_feature(X_test, method=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 전체로 fold_best_xgb_model 학습\n",
    "fold_best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# close 예측값 생성\n",
    "y_test_pred = fold_best_xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "# close 예측값을 등락폭에 따라 범주화\n",
    "y_test_pred_class = close_to_class(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_test_pred).to_csv(\"predicted_values_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 중요도 시각화 함수 정의\n",
    "def plot_xgb_importance(model):\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(24, 12))\n",
    "    axes = [ax for row_axes in axes for ax in row_axes]\n",
    "\n",
    "    plot_importance(model, max_num_features=20, importance_type=\"gain\", \n",
    "                    title=\"gain\", \n",
    "                    xlabel=\"\", \n",
    "                    grid=False,\n",
    "                    ax=axes[0])\n",
    "    plot_importance(model, max_num_features=20, importance_type=\"total_gain\", \n",
    "                    title=\"total_gain\", \n",
    "                    xlabel=\"\", \n",
    "                    grid=False,\n",
    "                    ax=axes[1])\n",
    "    plot_importance(model, max_num_features=20, importance_type=\"cover\", \n",
    "                    title=\"cover\", \n",
    "                    xlabel=\"\", \n",
    "                    grid=False,\n",
    "                    ax=axes[2])\n",
    "    plot_importance(model, max_num_features=20, importance_type=\"total_cover\", \n",
    "                    title=\"total_cover\", \n",
    "                    xlabel=\"\", \n",
    "                    grid=False,\n",
    "                    ax=axes[3])\n",
    "    plot_importance(model, max_num_features=20, importance_type=\"weight\", \n",
    "                    title=\"weight\", \n",
    "                    xlabel=\"\", \n",
    "                    grid=False,\n",
    "                    ax=axes[4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_xgb_importance(fold_best_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도 출력 (importance_type: gain)\n",
    "# 모델의 importance_type을 따로 지정하지 않았으면 default=gain\n",
    "feature_names = train_df.drop([\"ID\", \"target\", \"close\"], axis=1).columns\n",
    "importances = pd.Series(fold_best_xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "     print(importances)\n",
    "\n",
    "# 아래 코드 수정 필요 (ylabel이 고정되어있다.)\n",
    "# plt.barh(feature_names, importances)\n",
    "# for idx, val in zip(range(0,len(feature_names)), importances):\n",
    "#     plt.text(x=val, y=idx, s=importances[idx])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output File Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file 할당후 save \n",
    "submission_df = submission_df.assign(target = y_test_pred_class)\n",
    "submission_df[\"target\"] = submission_df[\"target\"].astype(np.int8)\n",
    "submission_df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Output Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv(\"output.csv\")\n",
    "out[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[\"target\"].value_counts(), '\\n')\n",
    "print(out[\"target\"].value_counts() / len(out[\"target\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
